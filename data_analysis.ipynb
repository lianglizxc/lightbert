{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_news = pd.read_excel('finance_data/SmoothNLP36kr新闻数据集10k.xlsx')\n",
    "finance_news = finance_news.drop(columns = ['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['content', 'insert_ts', 'pub_ts', 'title', 'url'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finance_news.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    如果你有一个产品，这个产品很有可能颠覆整个行业，但是这个产品可能需要你酝酿很久才能赚到钱，你...\n",
       "1    三星Galaxy S III这款手机已经吊足了很多人的胃口。最近国外网站\\nTweakers...\n",
       "2    今天晚上，甲骨文向Google索赔61亿美元的世纪侵权大案\\n将在旧金山联邦法院开审。预计8...\n",
       "3    Pinterest是一款很棒的产品，不仅一举成为\\n全美第三大社交网站，围绕\\n图片战略衍生...\n",
       "4    当苹果、Amazon以及它们的竞争对手们在全球就电子书定价问题打得不可开交之时，一家快速崛起...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finance_news['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'今天晚上，甲骨文向Google索赔61亿美元的世纪侵权大案\\n将在旧金山联邦法院开审。预计8周的时间后，将有一方潸然泪下。\\n审理过程中，Google CEO Larry Page、甲骨文CEO Larry Ellison、前Sun公司CEO Jonathan Schwartz和Google Android总裁Andy Rubin都将出庭作证。\\n该案审理的结果可能会产生非常深远的意义。我们一起来理清一下具体的情况：\\n专利问题\\n甲骨文最先于10年夏起诉Google称其拥有的Android系统侵犯了7项Java专利，索赔61亿美元。随后在专利检查中，有5项被剔除出去了，最后留下2项有争议的专利。\\nGoogle此前曾希望甲骨文放弃诉讼，作为回报其将抽出一部分Android所获利润再加上280万美元的一次性赔偿来补偿甲骨文（总计约1400万美元）。当然前提是甲骨文要在庭审中证明Android确实侵犯了2项Java专利，不过甲骨文拒绝了。\\n版权问题\\n甲骨文还将控诉Google在开发Android的过程中复制了超过37项Java API（application programming interfaces）以及11行源代码。这一控诉对于程序员来说具有深远影响，其判决结果将直接意味着今后开发者能否复制其他产品的API 。\\nGoogle则认为API不应该适用版权保护法，因为其更类似开发者用来开发软件的工具。更为简单的说就是Google认为一段具体的程序可以受到版权保护，而用来写程序的语言则不应该受到版权保护。\\n以上便是该诉讼的一些背景，相信庭审开始后会有更多的消息出来。\\n更新1（2012.5.24）：陪审团判决Android并未侵犯甲骨文专利，第2阶段审判结束。接下来将进行第3阶段审判，双方将主要就版权问题再度激战。\\n图片来源：\\nclash of titans'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finance_news['content'].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paddle enabled successfully......\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "jieba.enable_paddle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = jieba.cut(\"所以，整个过程有点像勘探。首先是有人发起，然后有千百人买了镐和铁锹跟进。有人会满载而归，但看走眼的人也会空手回来。\"\n",
    "                   ,cut_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhon.hanzi import punctuation as punctuation_zh\n",
    "from string import punctuation\n",
    "import collections\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stop_words():\n",
    "    stop_words = set()\n",
    "    with open('words_utils/stopwords','r') as file:\n",
    "        for word in file:\n",
    "            word = word.strip()\n",
    "            stop_words.add(word)\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = read_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JiebaTokenizer():\n",
    "    def __init__(self, stop_words, punctuations, vocab = None):\n",
    "        self.stop_words = stop_words\n",
    "        self.punctuation = set()\n",
    "        for punctuation in punctuations:\n",
    "            self.punctuation.update(set(punctuation))\n",
    "        self.vocab = self.init_vocab(vocab)\n",
    "            \n",
    "    def init_vocab(self, vocab):\n",
    "        init_vocab = {'<pad>':0,\n",
    "                 '<unk>':1,\n",
    "                 'CLS':2,\n",
    "                 'SEP':3,\n",
    "                 'MASK':4}\n",
    "        init_vocab = collections.OrderedDict(init_vocab)\n",
    "        if vocab is not None:\n",
    "            for word in vocab:\n",
    "                init_vocab[word] = len(init_vocab)\n",
    "        return init_vocab\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        tokens = []\n",
    "        for token in jieba.cut(text,cut_all=False):\n",
    "            if token in self.stop_words:\n",
    "                continue\n",
    "            if token in self.punctuation:\n",
    "                continue\n",
    "            if token == ' ' or token == '\\n' or token[0].isnumeric():\n",
    "                continue\n",
    "            if len(token) == 1 and token[0].isalpha():\n",
    "                continue\n",
    "            word = token.lower()\n",
    "            tokens.append(word)\n",
    "            self.vocab[word] = len(self.vocab)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = JiebaTokenizer(stop_words, [punctuation, punctuation_zh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_txt(finance_news):\n",
    "    with open('finance_data/data.txt','w') as file:\n",
    "        for index, row in finance_news.iterrows():\n",
    "            content = row['content']\n",
    "            try:\n",
    "                file.write(content)\n",
    "                file.write('\\n\\n')\n",
    "            except Exception as e:\n",
    "                print(content, e)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan write() argument must be str, not float\n",
      "nan write() argument must be str, not float\n"
     ]
    }
   ],
   "source": [
    "write_data_txt(finance_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from albertlib import albert_model, tokenization\n",
    "from albertlib.create_pretraining_data import create_training_instances, write_instance_to_example_files\n",
    "from albertlib.albert import AlbertConfig\n",
    "from albertlib.input_pipeline import create_pretrain_dataset\n",
    "import random\n",
    "from absl import logging, flags\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_data = {\n",
    "    \"max_seq_length\": 50,\n",
    "    \"max_predictions_per_seq\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:*** Reading from input files ***\n",
      "INFO:absl:  finance_data/data.txt\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-11f5fbde98a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0minput_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdupe_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_seq_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_lm_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_predictions_per_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m   rng)\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"number of instances: %i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pythonProject/lightbert/albertlib/create_pretraining_data.py\u001b[0m in \u001b[0;36mcreate_training_instances\u001b[0;34m(input_files, tokenizer, max_seq_length, dupe_factor, short_seq_prob, masked_lm_prob, max_predictions_per_seq, rng)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m           \u001b[0mall_documents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m           \u001b[0mall_documents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-e8de78341906>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjieba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcut_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/jieba/__init__.py\u001b[0m in \u001b[0;36mcut\u001b[0;34m(self, sentence, cut_all, HMM, use_paddle)\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mre_han\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcut_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/jieba/__init__.py\u001b[0m in \u001b[0;36m__cut_DAG\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mDAG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_DAG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mroute\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDAG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/jieba/__init__.py\u001b[0m in \u001b[0;36mcalc\u001b[0;34m(self, sentence, DAG, route)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mlogtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             route[idx] = max((log(self.FREQ.get(sentence[idx:x + 1]) or 1) -\n\u001b[0m\u001b[1;32m    178\u001b[0m                               logtotal + route[x + 1][0], x) for x in DAG[idx])\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logging.set_verbosity(logging.INFO)\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS.meta_data_file_path = 'processed_data/train_meta_data'\n",
    "FLAGS.input_file = 'finance_data/data.txt'\n",
    "FLAGS.max_seq_length = train_meta_data['max_seq_length']\n",
    "FLAGS.max_predictions_per_seq = train_meta_data['max_predictions_per_seq']\n",
    "FLAGS.output_file = 'processed_data/train.tf_record'\n",
    "FLAGS.dupe_factor = 40\n",
    "FLAGS.masked_lm_prob = 0.15\n",
    "FLAGS.short_seq_prob = 0.1\n",
    "FLAGS.mark_as_parsed()\n",
    "\n",
    "tokenizer = JiebaTokenizer(stop_words, [punctuation, punctuation_zh])\n",
    "\n",
    "input_files = []\n",
    "for input_pattern in FLAGS.input_file.split(\",\"):\n",
    "    input_files.extend(tf.io.gfile.glob(input_pattern))\n",
    "\n",
    "logging.info(\"*** Reading from input files ***\")\n",
    "for input_file in input_files:\n",
    "    logging.info(\"  %s\", input_file)\n",
    "\n",
    "rng = random.Random(FLAGS.random_seed)\n",
    "instances = create_training_instances(\n",
    "  input_files, tokenizer, FLAGS.max_seq_length, FLAGS.dupe_factor,\n",
    "  FLAGS.short_seq_prob, FLAGS.masked_lm_prob, FLAGS.max_predictions_per_seq,\n",
    "  rng)\n",
    "\n",
    "logging.info(\"number of instances: %i\", len(instances))\n",
    "\n",
    "output_files = FLAGS.output_file.split(\",\")\n",
    "logging.info(\"*** Writing to output files ***\")\n",
    "for output_file in output_files:\n",
    "    logging.info(\"  %s\", output_file)\n",
    "\n",
    "write_instance_to_example_files(instances, tokenizer, FLAGS.max_seq_length,\n",
    "                              FLAGS.max_predictions_per_seq, output_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
